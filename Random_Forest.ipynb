{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630cb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f7c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=(r\"C:\\Users\\mdirf\\Downloads\\Random Forest (1)\\Random Forest\\glass.xlsx\")\n",
    "df=pd.read_excel(file_path,sheet_name=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad65511",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(df.isnull().sum())\n",
    "print(df.describe())\n",
    "print(\"Number of duplicate rows:\", df.duplicated().sum())\n",
    "df=df.drop_duplicates()\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4dfa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr = df.corr()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71777d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Boxplots to detect outliers\n",
    "plt.figure(figsize=(15, 8))\n",
    "df.boxplot()\n",
    "plt.title(\"Boxplot for Outlier Detection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "\n",
    "z_scores = np.abs(zscore(df.select_dtypes(include=['int64', 'float64'])))\n",
    "outliers = (z_scores > 3).any(axis=1)\n",
    "print(\"Number of outliers detected:\", outliers.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c978b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Pairplot for feature relationships\n",
    "sns.pairplot(df, hue='Type', diag_kind='hist', corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b68496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for all numerical features\n",
    "df.hist(figsize=(12, 10), bins=20, color='skyblue')\n",
    "plt.suptitle('Histograms of Numerical Features', fontsize=16)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7dda27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e48f43be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Define imputers\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_imputer, num_cols),\n",
    "    ('cat', Pipeline(steps=[\n",
    "        ('imputer', cat_imputer),\n",
    "        ('onehot', OneHotEncoder(drop='first', dtype=float))\n",
    "    ]), cat_cols)\n",
    "])\n",
    "\n",
    "df_processed_array = preprocessor.fit_transform(df)\n",
    "\n",
    "all_features = preprocessor.get_feature_names_out()\n",
    "\n",
    "df_processed = pd.DataFrame(df_processed_array, columns=all_features)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_processed), columns=all_features)\n",
    "\n",
    "# Handle class imbalance if 'target' exists\n",
    "# if 'target' in df.columns:\n",
    "#     X = df_scaled\n",
    "#     y = df['target']\n",
    "#     smote = SMOTE(random_state=42)\n",
    "#     X_res, y_res = smote.fit_resample(X, y)\n",
    "    \n",
    "#     print(\"Original class distribution:\\n\", y.value_counts())\n",
    "#     print(\"Resampled class distribution:\\n\", y_res.value_counts())\n",
    "# else:\n",
    "#     X_res = df_scaled\n",
    "#     y_res = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "de646723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ae84857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      1.00      1.00        15\n",
      "           3       1.00      1.00      1.00         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         2\n",
      "           7       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00        43\n",
      "   macro avg       1.00      1.00      1.00        43\n",
      "weighted avg       1.00      1.00      1.00        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Assuming X_res and y_res are the processed features and target from previous steps\n",
    "# If target is not resampled, use X_scaled and y\n",
    "\n",
    "if y_res is not None:\n",
    "    X_final, y_final = X_res, y_res\n",
    "else:\n",
    "    X_final, y_final = X_res, df['Type']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_final, test_size=0.2, random_state=42, stratify=y_final\n",
    ")\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\\n\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7ff51d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target column\n",
    "target_col = 'Type'\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# if you want to balance classes using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# Now do train-test split safely\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6f96b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9bc11b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting Results:\n",
      "Accuracy: 0.8913043478260869\n",
      "Precision: 0.9012376689806357\n",
      "Recall: 0.8913043478260869\n",
      "F1 Score: 0.8926734778383048\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.80      0.73        15\n",
      "           2       0.93      0.87      0.90        15\n",
      "           3       0.92      0.73      0.81        15\n",
      "           5       1.00      0.94      0.97        16\n",
      "           6       1.00      1.00      1.00        16\n",
      "           7       0.88      1.00      0.94        15\n",
      "\n",
      "    accuracy                           0.89        92\n",
      "   macro avg       0.90      0.89      0.89        92\n",
      "weighted avg       0.90      0.89      0.89        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Boosting\n",
    "\n",
    "boosting_model = GradientBoostingClassifier(\n",
    "    n_estimators=100, learning_rate=0.1, random_state=42\n",
    ")\n",
    "boosting_model.fit(X_train, y_train)\n",
    "y_pred_boost = boosting_model.predict(X_test)\n",
    "\n",
    "print(\"Boosting Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_boost))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_boost, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_boost, average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_boost, average='weighted'))\n",
    "print(classification_report(y_test, y_pred_boost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63adebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Results:\n",
      "Accuracy: 0.8913043478260869\n",
      "Precision: 0.8959627329192547\n",
      "Recall: 0.8913043478260869\n",
      "F1 Score: 0.890432227678927\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.80      0.77        15\n",
      "           2       0.92      0.73      0.81        15\n",
      "           3       0.93      0.87      0.90        15\n",
      "           5       0.94      0.94      0.94        16\n",
      "           6       1.00      1.00      1.00        16\n",
      "           7       0.83      1.00      0.91        15\n",
      "\n",
      "    accuracy                           0.89        92\n",
      "   macro avg       0.89      0.89      0.89        92\n",
      "weighted avg       0.90      0.89      0.89        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bagging\n",
    "\n",
    "bagging_model = BaggingClassifier(\n",
    "    estimator=RandomForestClassifier(n_estimators=100, random_state=42), \n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_model.fit(X_train, y_train)\n",
    "y_pred_bag = bagging_model.predict(X_test)\n",
    "\n",
    "print(\"Bagging Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bag))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_bag, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_bag, average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_bag, average='weighted'))\n",
    "print(classification_report(y_test, y_pred_bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1817e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Bagging (Bootstrap Aggregating)\n",
    "\n",
    "# Bagging is an ensemble method that reduces variance by training multiple models on different random subsets \n",
    "# of the data (with replacement) and then aggregating their predictions.\n",
    "\n",
    "# How it works:\n",
    "# Randomly sample subsets of the training data with replacement (bootstrap samples).\n",
    "# Train a base model (like Decision Tree or Random Forest) on each subset.\n",
    "# Aggregate the predictions (majority vote for classification, average for regression).\n",
    "\n",
    "# Goal: Reduce overfitting and increase stability.\n",
    "# Example: Random Forest is a bagging method where multiple decision trees are trained on different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76281740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting\n",
    "\n",
    "# Boosting is an ensemble method that reduces bias by sequentially training models. Each model focuses on the errors made by the previous models.\n",
    "\n",
    "# How it works:\n",
    "# Train a weak learner (like a shallow tree).\n",
    "# Identify the data points the model predicted incorrectly.\n",
    "# Train the next model giving more weight to these misclassified points.\n",
    "# Combine all modelsâ€™ predictions with weighted voting (classification) or weighted sum (regression).\n",
    "\n",
    "# Goal: Improve model accuracy by focusing on mistakes.\n",
    "# Example: Gradient Boosting, AdaBoost, XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b7d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Handling Imbalanced Data\n",
    "\n",
    "# Imbalanced data occurs when one class significantly outnumbers others (e.g., 90% vs 10%). \n",
    "# This can make models biased toward the majority class.\n",
    "\n",
    "# Methods to handle imbalance\n",
    "# Resampling Techniques\n",
    "# Class Weights\n",
    "# Anomaly Detection / Specialized Metrics\n",
    "# Ensemble methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
